{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notes to help others. :) Good luck!\n",
    "\n",
    "In put image is 28 x 28 x 1, where each batch of stochastic Gradient Decent contains 16 (batch size). \n",
    "\n",
    "1) convolution 1: \n",
    "\n",
    "input: 16 x 28 x 28 x 1 \n",
    "  convolution is applied to image by using 5x5 patch, moving by 2x2 stride - so the image size decrease by factor of 2. \n",
    "  28 x 28 to 14 x 14\n",
    "  Additionally, depth (number of filters) is increased from 1 to 16, so the output is \n",
    "  16 x 14 x 14 x 16 = (# image x image_size x image_size x #filters)\n",
    "       \n",
    "2) convolution 2:  \n",
    "  \n",
    "input: 16 x 14 x 14 x 16\n",
    "  convolution is applied to image by using 5x5 patch, moving 2x2 stride - so the image size is reduced further by factor of 2. \n",
    "  14 x 14 to 7 x 7\n",
    "  Depth is kept the same, so number of filters remain as 16. \n",
    "  16 x 7 x 7 x 16\n",
    "  \n",
    "3) Hidden Network: \n",
    "\n",
    "input: 16 x (7 x 7 x 16) = 16 x (28/4 x 28/4 x 16) = 16 x 784\n",
    "   This is just like original deep learning data from assignment 2. \n",
    "   X*W + B = (16 x 784) * (784 * #nueral_node) + (16 x #nueral_node) = [16 x #nueral_node]\n",
    "   in below example: #nueral_node = num_hidden = 64\n",
    "  \n",
    "4) Output Network: \n",
    "  \n",
    "input: (16 x 64)\n",
    "    X * W + B = (16 x 64)*(64 x 10) + (16 x 10) = (16 x 10)\n",
    "    output is 10 vector (one-hot encoding of each class category) for each of 16 data point.\n",
    "'''\n",
    "\n",
    "def convDeepModel(batch_size,num_steps,L2_weight):\n",
    "    \n",
    "    image_size = 28\n",
    "    num_channels = 1 # grayscale\n",
    "    num_labels = 10  # classification\n",
    "    batch_size = 16  # number of data (image) per step for stochastic gradient decent\n",
    "    patch_size = 5   # 5x5 patch\n",
    "    depth = 16       # Number of filters or depth for convolution step\n",
    "    num_hidden = 64  # Hidden network for the last step\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, weights, biases):\n",
    "        \n",
    "        # Convolution layer 1\n",
    "        conv = tf.nn.conv2d(data, weights['conv1'],[1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['conv1'])\n",
    "        \n",
    "        # Convolution layer 2\n",
    "        conv = tf.nn.conv2d(hidden, weights['conv2'],[1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['conv2'])\n",
    "        \n",
    "        # Reshape output of Conv2 - to prepare as input of hidden layer 3\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        ## Hidden layer 3\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, weights['hidd3']) + biases['hidd3'])\n",
    "        \n",
    "        return tf.matmul(hidden, weights['out']) + biases['out']   \n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        # Input data. # shape = (16 x 28 x 28 x 1)\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32,shape=(batch_size, num_labels)) # shape = (16 x 10)\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'conv1': tf.Variable(tf.truncated_normal( \n",
    "                    [patch_size, patch_size, num_channels, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),          \n",
    "            'conv2': tf.Variable(tf.truncated_normal(\n",
    "                    [patch_size, patch_size, depth, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),         \n",
    "            'hidd3': tf.Variable(tf.truncated_normal(\n",
    "                    [image_size // 4 * image_size // 4 * depth, num_hidden], \n",
    "                    stddev=0.1)\n",
    "                                ),\n",
    "            'out': tf.Variable(tf.truncated_normal(\n",
    "                    [num_hidden, num_labels], \n",
    "                    stddev=0.1)\n",
    "                              )\n",
    "        }\n",
    "        biases = {\n",
    "            'conv1': tf.Variable(tf.zeros([depth])),\n",
    "            'conv2': tf.Variable(tf.constant(1.0, shape=[depth])),\n",
    "            'hidd3': tf.Variable(tf.constant(1.0, shape=[num_hidden])),\n",
    "            'out': tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "        }\n",
    "        \n",
    "        # Training computation.\n",
    "        logits = model(tf_train_dataset, weights, biases)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset, weights, biases))\n",
    "        test_prediction = tf.nn.softmax(model(tf_test_dataset, weights, biases))\n",
    "\n",
    "    num_steps = 1001\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            if (step % 200 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.460344\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 12.6%\n",
      "Minibatch loss at step 200: 0.510356\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 400: 0.727496\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 600: 0.617148\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 800: 0.328495\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1000: 0.794244\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.4%\n"
     ]
    }
   ],
   "source": [
    "convDeepModel(batch_size=16,num_steps=1001,L2_weight=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.935316\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 200: 0.597066\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 400: 0.791855\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 600: 0.446352\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 800: 0.202790\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1000: 0.751617\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.1%\n",
      "Test accuracy: 89.7%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notes to help others. :) Good luck!\n",
    "\n",
    "Added 3rd convolution layer. \n",
    "I kept the dept same (16 = number of filters) and used stride of 1x1 so images of 7x7 (from previous layer) \n",
    "do get further reduced. Well... since 7 is a prime number, you can't really do any better than that! I do \n",
    "not think adding another really helps in anyway at all, since I can't reduce image size any futher. \n",
    "'''\n",
    "\n",
    "def convDeepModel_2(batch_size,num_steps,L2_weight):\n",
    "    \n",
    "    image_size = 28\n",
    "    num_channels = 1 # grayscale\n",
    "    num_labels = 10  # classification\n",
    "    batch_size = 16  # number of data (image) per step for stochastic gradient decent\n",
    "    patch_size = 5   # 5x5 patch\n",
    "    depth = 16       # Number of filters or depth for convolution step\n",
    "    num_hidden = 64  # Hidden network for the last step\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, weights, biases):\n",
    "        \n",
    "        # Convolution layer 1\n",
    "        conv = tf.nn.conv2d(data, weights['conv1'],[1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['conv1'])\n",
    "        \n",
    "        # Convolution layer 2\n",
    "        conv = tf.nn.conv2d(hidden, weights['conv2'],[1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['conv2'])\n",
    "        \n",
    "        # Convolution layer 3\n",
    "        conv = tf.nn.conv2d(hidden, weights['conv3'],[1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['conv3'])\n",
    "        \n",
    "        # Reshape output of Conv3 - to prepare as input of hidden layer 3\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        ## Hidden layer 3\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, weights['hidd3']) + biases['hidd3'])\n",
    "        \n",
    "        return tf.matmul(hidden, weights['out']) + biases['out']   \n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        # Input data. # shape = (16 x 28 x 28 x 1)\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32,shape=(batch_size, num_labels)) # shape = (16 x 10)\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'conv1': tf.Variable(tf.truncated_normal( \n",
    "                    [patch_size, patch_size, num_channels, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),          \n",
    "            'conv2': tf.Variable(tf.truncated_normal(\n",
    "                    [patch_size, patch_size, depth, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),  \n",
    "            'conv3': tf.Variable(tf.truncated_normal(\n",
    "                    [patch_size, patch_size, depth, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),  \n",
    "            'hidd3': tf.Variable(tf.truncated_normal(\n",
    "                    [image_size // 4 * image_size // 4 * depth, num_hidden], \n",
    "                    stddev=0.1)\n",
    "                                ),\n",
    "            'out': tf.Variable(tf.truncated_normal(\n",
    "                    [num_hidden, num_labels], \n",
    "                    stddev=0.1)\n",
    "                              )\n",
    "        }\n",
    "        biases = {\n",
    "            'conv1': tf.Variable(tf.zeros([depth])),\n",
    "            'conv2': tf.Variable(tf.constant(1.0, shape=[depth])),\n",
    "            'conv3': tf.Variable(tf.constant(1.0, shape=[depth])),\n",
    "            'hidd3': tf.Variable(tf.constant(1.0, shape=[num_hidden])),\n",
    "            'out': tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "        }\n",
    "        \n",
    "        # Training computation.\n",
    "        logits = model(tf_train_dataset, weights, biases)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset, weights, biases))\n",
    "        test_prediction = tf.nn.softmax(model(tf_test_dataset, weights, biases))\n",
    "\n",
    "    num_steps = 1001\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            if (step % 200 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "        \n",
    "convDeepModel_2(batch_size=16,num_steps=1001,L2_weight=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "One can probably do better by adding methods such as dropout or L2 regularization and doing PCA, on the image before proceding with convolution. Also, one can add more hidden layers If one want to.  \n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convDeepModel_3(batch_size,num_steps,L2_weight):\n",
    "    \n",
    "    image_size = 28\n",
    "    num_channels = 1 # grayscale\n",
    "    num_labels = 10  # classification\n",
    "    patch_size = 5   # 5x5 patch\n",
    "    depth = 16       # Number of filters or depth for convolution step\n",
    "    num_hidden = 64  # Hidden network for the last step\n",
    "\n",
    "    def conv2d(x, W, b, strides=1):\n",
    "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    def maxpool2d(x, k=2):\n",
    "        # MaxPool2D wrapper\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], \n",
    "                              strides=[1, k, k, 1],\n",
    "                              padding='SAME')\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, weights, biases):\n",
    "        \n",
    "        # Convolution layer 1\n",
    "        conv = conv2d(data, weights['conv1'],biases['conv1'],strides=1)\n",
    "        pool = maxpool2d(conv, k=2)\n",
    "        \n",
    "        # Convolution layer 2\n",
    "        conv = conv2d(pool, weights['conv2'],biases['conv2'],strides=1)\n",
    "        pool = maxpool2d(conv, k=2)\n",
    "        \n",
    "        # Reshape output of Conv2 - to prepare as input of hidden layer 3\n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        ## Hidden layer 3\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, weights['hidd3']) + biases['hidd3'])\n",
    "        \n",
    "        return tf.matmul(hidden, weights['out']) + biases['out']   \n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        # Input data. # shape = (16 x 28 x 28 x 1)\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32,shape=(batch_size, num_labels)) # shape = (16 x 10)\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'conv1': tf.Variable(tf.truncated_normal( \n",
    "                    [patch_size, patch_size, num_channels, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),          \n",
    "            'conv2': tf.Variable(tf.truncated_normal(\n",
    "                    [patch_size, patch_size, depth, depth], \n",
    "                    stddev=0.1)\n",
    "                                ),         \n",
    "            'hidd3': tf.Variable(tf.truncated_normal(\n",
    "                    [7 * 7 * depth, num_hidden], \n",
    "                    stddev=0.1)\n",
    "                                ),\n",
    "            'out': tf.Variable(tf.truncated_normal(\n",
    "                    [num_hidden, num_labels], \n",
    "                    stddev=0.1)\n",
    "                              )\n",
    "        }\n",
    "        biases = {\n",
    "            'conv1': tf.Variable(tf.zeros([depth])),\n",
    "            'conv2': tf.Variable(tf.constant(1.0, shape=[depth])),\n",
    "            'hidd3': tf.Variable(tf.constant(1.0, shape=[num_hidden])),\n",
    "            'out': tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "        }\n",
    "        \n",
    "        # Training computation.\n",
    "        logits = model(tf_train_dataset, weights, biases)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset, weights, biases))\n",
    "        test_prediction = tf.nn.softmax(model(tf_test_dataset, weights, biases))\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            if (step % 200 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.576232\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 200: 0.463078\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 400: 0.709035\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 600: 0.532466\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 800: 0.390249\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1000: 0.718362\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1200: 0.086665\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1400: 0.576786\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1600: 0.866188\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1800: 0.484554\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 0.716943\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 92.3%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 4.106421\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 9.2%\n",
      "Minibatch loss at step 200: 0.714123\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 400: 0.676473\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 600: 0.349990\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 800: 0.367415\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1000: 0.625030\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.6%\n",
      "Test accuracy: 90.8%\n"
     ]
    }
   ],
   "source": [
    "convDeepModel_3(batch_size=16,num_steps=2001,L2_weight=0.0)\n",
    "convDeepModel_3(batch_size=16,num_steps=1001,L2_weight=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
