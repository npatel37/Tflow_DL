{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train1Dlabels = []\n",
    "Valid1Dlabels = []\n",
    "Test1Dlabels = []\n",
    "\n",
    "def label_data(data):\n",
    "    Tlabels = []\n",
    "    for label in data:\n",
    "        truelabel = [i for i, x in enumerate(label) if x]\n",
    "        Tlabels.append(truelabel[0])\n",
    "    return Tlabels\n",
    "    \n",
    "Train1Dlabels = label_data(train_labels)\n",
    "Valid1Dlabels = label_data(valid_labels)\n",
    "Test1Dlabels = label_data(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0.2846 0.302\n",
      "0.0001 0.2288 0.248\n",
      "0.001 0.1936 0.226\n",
      "0.002 0.1838 0.212\n",
      "0.003 0.1782 0.212\n",
      "0.004 0.1726 0.208\n",
      "0.005 0.171 0.208\n",
      "0.006 0.1684 0.206\n",
      "0.007 0.1666 0.208\n",
      "0.008 0.1644 0.208\n",
      "0.009 0.1628 0.206\n",
      "0.01 0.1624 0.204\n",
      "0.01 0.1624 0.204\n",
      "0.02 0.149 0.208\n",
      "0.03 0.1402 0.206\n",
      "0.04 0.135 0.202\n",
      "0.05 0.13 0.2\n",
      "0.06 0.1266 0.2\n",
      "0.07 0.1238 0.198\n",
      "0.08 0.1198 0.198\n",
      "0.09 0.1184 0.198\n",
      "0.1 0.1148 0.198\n",
      "0.2 0.0956 0.2\n",
      "0.3 0.0824 0.206\n",
      "0.4 0.073 0.214\n",
      "0.5 0.068 0.22\n",
      "0.6 0.062 0.22\n",
      "0.7 0.0588 0.224\n",
      "0.8 0.0554 0.224\n",
      "0.9 0.0518 0.226\n",
      "1.0 0.0498 0.228\n",
      "10.0 0.0086 0.258\n"
     ]
    }
   ],
   "source": [
    "X = train_dataset[:5000]\n",
    "Y = Train1Dlabels[:5000]\n",
    "X_valid = valid_dataset[:500]\n",
    "Y_valid = Valid1Dlabels[:500]\n",
    "\n",
    "L2_weights = [0.00001, 0.0001, 0.001, 0.002, 0.003, 0.004, \n",
    "              0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.01,\n",
    "              0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09,\n",
    "              0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 10.0]\n",
    "Verror = []\n",
    "Trerror = []\n",
    "for weight in L2_weights:\n",
    "    regr = Linear.LogisticRegression(C=weight)\n",
    "    regr.fit(X,Y)\n",
    "    Verror.append(1.0 - regr.score(X_valid,Y_valid));\n",
    "    Trerror.append(1.0 - regr.score(X,Y))\n",
    "    print (weight, 1.0 - regr.score(X,Y), 1.0 - regr.score(X_valid,Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEECAYAAADOJIhPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAiIiOez/AcKAwBmGM1qJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4f55d1690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(L2_weights,Trerror, \"b^-\", label = \"Training\")\n",
    "plt.plot(L2_weights,Verror, \"ro--\", label = \"Validation\")\n",
    "plt.xscale('log')\n",
    "plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### - Plot above show that the minima in validation error accors when L2-penalty weight ~ 0.085\n",
    "### - Lets see if the crossvalidation agree with us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = Linear.LogisticRegressionCV()\n",
    "regr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04641589  0.04641589  0.04641589  0.04641589  0.04641589  0.04641589\n",
      "  0.04641589  0.04641589  0.00599484  0.04641589]\n"
     ]
    }
   ],
   "source": [
    "print (regr.C_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two L2-panelties ~= 0.04641589, 0.006\n",
    "#### so, lets try all of them, see which one is the best one.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_nodes = 1024\n",
    "L2_weight = 0.085\n",
    "num_steps = 3001\n",
    "\n",
    "def tf_modelRegr(batch_size,n_nodes,num_steps,L2_weight):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels =  tf.placeholder(tf.float32,shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # input layer - (784 col) * (784 row x 1024 col) + (1024 col) = (1024 col)\n",
    "        weights_01 = tf.Variable(tf.truncated_normal([image_size * image_size, n_nodes]))\n",
    "        biases_01 = tf.Variable(tf.zeros([n_nodes]))\n",
    "\n",
    "        # then hidden layer - rectified linear on output of input layer\n",
    "        # output layer - (1024 col) * (1024 row x 10 col) + (10 col) = (10 col)\n",
    "        weights_12 = tf.Variable(tf.truncated_normal([n_nodes, num_labels]))\n",
    "        biases_12 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        logits_01 = tf.matmul(tf_train_dataset, weights_01) + biases_01   ## input layer\n",
    "        h1 = tf.nn.relu(logits_01) ## Hidden layer\n",
    "        logits_12 = tf.matmul(h1, weights_12) + biases_12     ## output layer\n",
    "\n",
    "        ## calculating loss function\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_12) + \n",
    "            L2_weight*tf.nn.l2_loss(weights_01) + \n",
    "            L2_weight*tf.nn.l2_loss(weights_12)\n",
    "        )\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # Predictions for the training\n",
    "        train_prediction = tf.nn.softmax(logits_12) \n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_01) + biases_01), weights_12) + biases_12\n",
    "        )\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_01) + biases_01), weights_12) + biases_12\n",
    "        )\n",
    " \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#             if (step % 500 == 0):\n",
    "#                 print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#                 print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#                 print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#                 valid_prediction.eval(), valid_labels))\n",
    "        #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        print(L2_weight, \n",
    "              accuracy(valid_prediction.eval(), valid_labels), \n",
    "              accuracy(test_prediction.eval(), test_labels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005 84.09 90.87\n",
      "0.00075 85.82 92.21\n",
      "0.0008 86.24 92.28\n",
      "0.0009 86.58 92.85\n",
      "0.001 86.77 93.02\n",
      "0.002 86.25 92.41\n",
      "0.0025 86.03 92.23\n",
      "0.005 84.61 90.81\n",
      "0.0075 83.68 90.03\n"
     ]
    }
   ],
   "source": [
    "weights = [0.0005,0.00075,0.0008,0.0009,0.001,0.002,0.0025,0.005,0.0075]\n",
    "for L2_W in weights:\n",
    "    tf_modelRegr(batch_size=128,n_nodes=1024,num_steps=3001,L2_weight=L2_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### - L2 weight which maximize the validation and test error is 0.001 \n",
    "### - maximum test accuracy = 93.02%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def tf_modelRegr_overfit(batch_size,n_nodes,num_steps,L2_weight):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels =  tf.placeholder(tf.float32,shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # input layer - (784 col) * (784 row x 1024 col) + (1024 col) = (1024 col)\n",
    "        weights_01 = tf.Variable(tf.truncated_normal([image_size * image_size, n_nodes]))\n",
    "        biases_01 = tf.Variable(tf.zeros([n_nodes]))\n",
    "\n",
    "        # then hidden layer - rectified linear on output of input layer\n",
    "        # output layer - (1024 col) * (1024 row x 10 col) + (10 col) = (10 col)\n",
    "        weights_12 = tf.Variable(tf.truncated_normal([n_nodes, num_labels]))\n",
    "        biases_12 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        logits_01 = tf.matmul(tf_train_dataset, weights_01) + biases_01   ## input layer\n",
    "        h1 = tf.nn.relu(logits_01) ## Hidden layer\n",
    "        logits_12 = tf.matmul(h1, weights_12) + biases_12     ## output layer\n",
    "\n",
    "        ## calculating loss function\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_12) + \n",
    "            L2_weight*tf.nn.l2_loss(weights_01) + \n",
    "            L2_weight*tf.nn.l2_loss(weights_12)\n",
    "        )\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # Predictions for the training\n",
    "        train_prediction = tf.nn.softmax(logits_12) \n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_01) + biases_01), weights_12) + biases_12\n",
    "        )\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_01) + biases_01), weights_12) + biases_12\n",
    "        )\n",
    "        \n",
    "    Tdata = train_dataset[:20000]\n",
    "    Tlabel = train_labels[:20000]\n",
    " \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (Tlabel.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = Tdata[offset:(offset + batch_size), :]\n",
    "            batch_labels = Tlabel[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "        #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        print(L2_weight, \n",
    "              accuracy(valid_prediction.eval(), valid_labels), \n",
    "              accuracy(test_prediction.eval(), test_labels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 634.014832\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 35.8%\n",
      "Minibatch loss at step 500: 192.429443\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1000: 115.044525\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 69.354401\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2000: 42.004883\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2500: 25.494728\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 3000: 15.564071\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 84.3%\n",
      "0.001 84.28 91.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_nodes = 1024\n",
    "L2_weight = 0.085\n",
    "num_steps = 3001\n",
    "tf_modelRegr_overfit(batch_size=128,n_nodes=1024,num_steps=3001,L2_weight=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overfit - note that the minibatch (training) accuracy is nearly 100%, a clear sign of overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tf_modelRegr_overfit(batch_size,n_nodes,num_steps,L2_weight):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels =  tf.placeholder(tf.float32,shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # input layer - (784 col) * (784 row x 1024 col) + (1024 col) = (1024 col)\n",
    "        weights_01 = tf.Variable(tf.truncated_normal([image_size * image_size, n_nodes]))\n",
    "        biases_01 = tf.Variable(tf.zeros([n_nodes]))\n",
    "\n",
    "        # then hidden layer - rectified linear on output of input layer\n",
    "        # output layer - (1024 col) * (1024 row x 10 col) + (10 col) = (10 col)\n",
    "        weights_12 = tf.Variable(tf.truncated_normal([n_nodes, num_labels]))\n",
    "        biases_12 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        logits_01 = tf.matmul(tf_train_dataset, weights_01) + biases_01   ## input layer\n",
    "        h1 = tf.nn.relu(logits_01) ## Hidden layer\n",
    "        h1_drop = tf.nn.dropout(h1,0.50)\n",
    "        logits_12 = tf.matmul(h1_drop, weights_12) + biases_12     ## output layer\n",
    "\n",
    "        ## calculating loss function\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_12) + \n",
    "            L2_weight*tf.nn.l2_loss(weights_01) + \n",
    "            L2_weight*tf.nn.l2_loss(weights_12)\n",
    "        )\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # Predictions for the training\n",
    "        train_prediction = tf.nn.softmax(logits_12) \n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_01) + biases_01), weights_12) + biases_12\n",
    "        )\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_01) + biases_01), weights_12) + biases_12\n",
    "        )\n",
    "        \n",
    "    Tdata = train_dataset[:20000]\n",
    "    Tlabel = train_labels[:20000]\n",
    " \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (Tlabel.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = Tdata[offset:(offset + batch_size), :]\n",
    "            batch_labels = Tlabel[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "        #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        print(\"L2_Weight, Validation Error, Test Error\")\n",
    "        print(L2_weight, \n",
    "              accuracy(valid_prediction.eval(), valid_labels), \n",
    "              accuracy(test_prediction.eval(), test_labels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 817.575928\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 500: 198.934082\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1000: 117.691956\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1500: 70.531876\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2000: 42.372993\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2500: 25.687977\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3000: 15.782421\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.9%\n",
      "L2_Weight, Validation Error, Test Error\n",
      "0.001 84.89 91.67\n"
     ]
    }
   ],
   "source": [
    "tf_modelRegr_overfit(batch_size=128,n_nodes=1024,num_steps=3001,L2_weight=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am not sure if I implemented the dropout properly.. But.. The minibatch accuracy (training error) seems to be better controlled than before and \"might\" not be over fitted. Also, test error is 91.67 which is preety good for only 20000 data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try net with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784    # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10   # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "def multilayer_perceptron(data, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(data, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer 2 with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def tfDeep_modelRegr(batch_size,num_steps,L2_weight):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, n_input))\n",
    "        tf_train_labels =  tf.placeholder(tf.float32,shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1], \n",
    "                                                  stddev=np.sqrt(2.0 /float(n_input)))\n",
    "                              ),\n",
    "            'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],\n",
    "                                                 stddev=np.sqrt(2.0 /float(n_hidden_1)))\n",
    "                             ),\n",
    "            'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],\n",
    "                                                  stddev=np.sqrt(2.0 /float(n_hidden_2)))\n",
    "                              )\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "            'out': tf.Variable(tf.zeros([n_classes]))\n",
    "        }\n",
    "\n",
    "        # Construct model\n",
    "        pred = multilayer_perceptron(tf_train_dataset, weights, biases)\n",
    "\n",
    "        ## calculating loss function\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=pred) + \n",
    "            L2_weight*tf.nn.l2_loss(weights['h1']) + \n",
    "            L2_weight*tf.nn.l2_loss(weights['h2']) +\n",
    "            L2_weight*tf.nn.l2_loss(weights['out'])\n",
    "        )\n",
    "        \n",
    "        # Optimizer.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training\n",
    "        train_prediction = tf.nn.softmax(pred) \n",
    "        valid_prediction = tf.nn.softmax(multilayer_perceptron(tf_valid_dataset, weights, biases))\n",
    "        test_prediction = tf.nn.softmax(multilayer_perceptron(tf_test_dataset, weights, biases))\n",
    "        \n",
    " \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            \n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "            if (step % 500 == 0):\n",
    "                #print(predictions)\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                \n",
    "        #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        print(L2_weight, \n",
    "              accuracy(valid_prediction.eval(), valid_labels), \n",
    "              accuracy(test_prediction.eval(), test_labels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.767899\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 36.4%\n",
      "Minibatch loss at step 500: 0.741792\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1000: 0.579215\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 1500: 0.684901\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000: 0.538576\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2500: 0.559691\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 3000: 0.496582\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "0.001 88.44 94.29\n"
     ]
    }
   ],
   "source": [
    "tfDeep_modelRegr(batch_size=128,num_steps=3001,L2_weight=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try now with 5 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784    # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10   # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "def multilayer_perceptron(data, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(data, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer 2 with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def tfDeep_modelRegr(batch_size,num_steps,L2_weight):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, n_input))\n",
    "        tf_train_labels =  tf.placeholder(tf.float32,shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1], \n",
    "                                                  stddev=np.sqrt(2.0 /float(n_input)))\n",
    "                              ),\n",
    "            'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],\n",
    "                                                 stddev=np.sqrt(2.0 /float(n_hidden_1)))\n",
    "                             ),\n",
    "            'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],\n",
    "                                                  stddev=np.sqrt(2.0 /float(n_hidden_2)))\n",
    "                              )\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "            'out': tf.Variable(tf.zeros([n_classes]))\n",
    "        }\n",
    "\n",
    "        # Construct model\n",
    "        pred = multilayer_perceptron(tf_train_dataset, weights, biases)\n",
    "\n",
    "        ## calculating loss function\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=pred) + \n",
    "            L2_weight*tf.nn.l2_loss(weights['h1']) + \n",
    "            L2_weight*tf.nn.l2_loss(weights['h2']) +\n",
    "            L2_weight*tf.nn.l2_loss(weights['out'])\n",
    "        )\n",
    "        \n",
    "        # Optimizer.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training\n",
    "        train_prediction = tf.nn.softmax(pred) \n",
    "        valid_prediction = tf.nn.softmax(multilayer_perceptron(tf_valid_dataset, weights, biases))\n",
    "        test_prediction = tf.nn.softmax(multilayer_perceptron(tf_test_dataset, weights, biases))\n",
    "        \n",
    " \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            \n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "            if (step % 500 == 0):\n",
    "                #print(predictions)\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                \n",
    "        #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        print(L2_weight, \n",
    "              accuracy(valid_prediction.eval(), valid_labels), \n",
    "              accuracy(test_prediction.eval(), test_labels))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
